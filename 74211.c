static int CVE_2013_6380_PATCHED_aac_send_raw_srb(struct aac_dev* dev, void __user * arg) struct fib * srbfib ; struct user_aac_srb __user * user_srb = arg ; u32 fibsize = 0 ; if ( dev -> in_reset )  if ( ! capable ( CAP_SYS_ADMIN ) )  if ( ! ( srbfib = aac_fib_alloc ( dev ) ) )  srbfib -> hw_fib_va -> header . XferState &= ~cpu_to_le32 ( FastResponseCapable ); srbcmd = ( struct aac_srb * ) fib_data ( srbfib ); if ( copy_from_user ( & fibsize , & user_srb -> count , sizeof ( u32 ) ) )  if ( ( fibsize < ( sizeof ( struct user_aac_srb ) - sizeof ( struct user_sgentry ) ) ) || ( fibsize > ( dev -> max_fib_size - sizeof ( struct aac_fibhdr ) ) ) )  user_srbcmd = kmalloc ( fibsize , GFP_KERNEL ); if ( ! user_srbcmd )  if ( copy_from_user ( user_srbcmd , user_srb , fibsize ) )  flags = user_srbcmd -> flags; srbcmd -> function = cpu_to_le32 ( SRBF_ExecuteScsi ); srbcmd -> channel = cpu_to_le32 ( user_srbcmd -> channel ); srbcmd -> id = cpu_to_le32 ( user_srbcmd -> id ); srbcmd -> lun = cpu_to_le32 ( user_srbcmd -> lun ); srbcmd -> timeout = cpu_to_le32 ( user_srbcmd -> timeout ); srbcmd -> flags = cpu_to_le32 ( flags ); srbcmd -> retry_limit = 0; srbcmd -> cdb_size = cpu_to_le32 ( user_srbcmd -> cdb_size ); memcpy ( srbcmd -> cdb , user_srbcmd -> cdb , sizeof ( srbcmd -> cdb ) ); dprintk ( ( KERN_DEBUG "aacraid: too many sg entries %d\n" le32_to_cpu ( srbcmd -> sg . count ) ) ) struct sgmap64 * psg = ( struct sgmap64 * ) & srbcmd -> sg ; if ( upsg -> sg [ i ] . count > ( ( dev -> adapter_info . options & AAC_OPT_NEW_COMM ) ? ( dev -> scsi_host_ptr -> max_sectors << 9 ) : 65536 ) )  p = kmalloc ( upsg -> sg [ i ] . count , GFP_KERNEL | __GFP_DMA ); if ( ! p )  addr = ( u64 ) upsg -> sg [ i ] . addr [ 0 ]; addr += ( ( u64 ) upsg -> sg [ i ] . addr [ 1 ] ) << 32; sg_user [ i ] = ( void __user * ) ( uintptr_t ) addr; sg_list [ i ] = p; sg_indx = i; if ( copy_from_user ( p , sg_user [ i ] , upsg -> sg [ i ] . count ) )  addr = pci_map_single ( dev -> pdev , p , upsg -> sg [ i ] . count , data_dir ); psg -> sg [ i ] . addr [ 0 ] = cpu_to_le32 ( addr & 0xffffffff ); psg -> sg [ i ] . addr [ 1 ] = cpu_to_le32 ( addr >> 32 ); byte_count += upsg -> sg [ i ] . count; psg -> sg [ i ] . count = cpu_to_le32 ( upsg -> sg [ i ] . count ); if ( usg -> sg [ i ] . count > ( ( dev -> adapter_info . options & AAC_OPT_NEW_COMM ) ? ( dev -> scsi_host_ptr -> max_sectors << 9 ) : 65536 ) )  p = kmalloc ( usg -> sg [ i ] . count , GFP_KERNEL | __GFP_DMA ); if ( ! p )  sg_user [ i ] = ( void __user * ) ( uintptr_t ) usg -> sg [ i ] . addr; sg_list [ i ] = p; sg_indx = i; if ( copy_from_user ( p , sg_user [ i ] , upsg -> sg [ i ] . count ) )  addr = pci_map_single ( dev -> pdev , p , usg -> sg [ i ] . count , data_dir ); psg -> sg [ i ] . addr [ 0 ] = cpu_to_le32 ( addr & 0xffffffff ); psg -> sg [ i ] . addr [ 1 ] = cpu_to_le32 ( addr >> 32 ); byte_count += usg -> sg [ i ] . count; psg -> sg [ i ] . count = cpu_to_le32 ( usg -> sg [ i ] . count ); srbcmd -> count = cpu_to_le32 ( byte_count ); psg -> count = cpu_to_le32 ( sg_indx + 1 ); for(i = 0 ; i <= sg_indx; i++) byte_count = le32_to_cpu ( ( dev -> adapter_info . options & AAC_OPT_SGMAP_HOST64 ) ? ( ( struct sgmap64 * ) & srbcmd -> sg ) -> sg [ i ] . count : srbcmd -> sg . sg [ i ] . count ); if ( copy_to_user ( sg_user [ i ] , sg_list [ i ] , byte_count ) )  for(i=0; i <= sg_indx; i++) kfree ( sg_list [ i ] ); 