static __init int hardware_setup(void) vmx_io_bitmap_a = ( unsigned long * ) __get_free_page ( GFP_KERNEL ); if ( ! vmx_io_bitmap_a )  vmx_io_bitmap_b = ( unsigned long * ) __get_free_page ( GFP_KERNEL ); if ( ! vmx_io_bitmap_b )  vmx_msr_bitmap_legacy = ( unsigned long * ) __get_free_page ( GFP_KERNEL ); if ( ! vmx_msr_bitmap_legacy )  vmx_msr_bitmap_legacy_x2apic = ( unsigned long * ) __get_free_page ( GFP_KERNEL ); if ( ! vmx_msr_bitmap_legacy_x2apic )  vmx_msr_bitmap_longmode = ( unsigned long * ) __get_free_page ( GFP_KERNEL ); if ( ! vmx_msr_bitmap_longmode )  vmx_msr_bitmap_longmode_x2apic = ( unsigned long * ) __get_free_page ( GFP_KERNEL ); if ( ! vmx_msr_bitmap_longmode_x2apic )  if ( nested )  vmx_msr_bitmap_nested = ( unsigned long * ) __get_free_page ( GFP_KERNEL ); if ( ! vmx_msr_bitmap_nested )  vmx_vmread_bitmap = ( unsigned long * ) __get_free_page ( GFP_KERNEL ); if ( ! vmx_vmread_bitmap )  vmx_vmwrite_bitmap = ( unsigned long * ) __get_free_page ( GFP_KERNEL ); if ( ! vmx_vmwrite_bitmap )  if ( setup_vmcs_config ( & vmcs_config ) < 0 )  static __init int setup_vmcs_config(struct vmcs_config *vmcs_conf) u32 vmx_msr_low , vmx_msr_high ; u32 min , opt , min2 , opt2 ; u32 _cpu_based_exec_control = 0 ; min = CPU_BASED_HLT_EXITING | opt = CPU_BASED_TPR_SHADOW | CPU_BASED_USE_MSR_BITMAPS | CPU_BASED_ACTIVATE_SECONDARY_CONTROLS; if ( adjust_vmx_controls ( min , opt , MSR_IA32_VMX_PROCBASED_CTLS , & _cpu_based_exec_control ) < 0 )  static __init int adjust_vmx_controls(u32 ctl_min, u32 u32 msr, u32 *result) if ( ctl_min & ~ctl )  return - EIO ; return 0 ; return - EIO ; if ( _cpu_based_exec_control & CPU_BASED_TPR_SHADOW )  _cpu_based_exec_control &= ~CPU_BASED_CR8_LOAD_EXITING & ~CPU_BASED_CR8_STORE_EXITING; if ( _cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS )  min2 = 0; opt2 = SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES | SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE | SECONDARY_EXEC_WBINVD_EXITING | SECONDARY_EXEC_ENABLE_VPID | SECONDARY_EXEC_ENABLE_EPT | SECONDARY_EXEC_UNRESTRICTED_GUEST | SECONDARY_EXEC_PAUSE_LOOP_EXITING | SECONDARY_EXEC_RDTSCP | SECONDARY_EXEC_ENABLE_INVPCID | SECONDARY_EXEC_APIC_REGISTER_VIRT | SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY | SECONDARY_EXEC_SHADOW_VMCS | SECONDARY_EXEC_XSAVES | SECONDARY_EXEC_ENABLE_PML; if ( adjust_vmx_controls ( min2 , opt2 , MSR_IA32_VMX_PROCBASED_CTLS2 , & _cpu_based_2nd_exec_control ) < 0 )  static __init int adjust_vmx_controls(u32 ctl_min, u32 u32 msr, u32 *result) if ( ctl_min & ~ctl )  return - EIO ; return 0 ; return - EIO ; min = VM_EXIT_SAVE_DEBUG_CONTROLS; min |= VM_EXIT_HOST_ADDR_SPACE_SIZE; opt = VM_EXIT_SAVE_IA32_PAT | VM_EXIT_LOAD_IA32_PAT | VM_EXIT_ACK_INTR_ON_EXIT | VM_EXIT_CLEAR_BNDCFGS; if ( adjust_vmx_controls ( min , opt , MSR_IA32_VMX_EXIT_CTLS , & _vmexit_control ) < 0 )  static __init int adjust_vmx_controls(u32 ctl_min, u32 u32 msr, u32 *result) if ( ctl_min & ~ctl )  return - EIO ; return 0 ; return - EIO ; min = PIN_BASED_EXT_INTR_MASK | PIN_BASED_NMI_EXITING; opt = PIN_BASED_VIRTUAL_NMIS | PIN_BASED_POSTED_INTR; if ( adjust_vmx_controls ( min , opt , MSR_IA32_VMX_PINBASED_CTLS , & _pin_based_exec_control ) < 0 )  static __init int adjust_vmx_controls(u32 ctl_min, u32 u32 msr, u32 *result) if ( ctl_min & ~ctl )  return - EIO ; return 0 ; return - EIO ; min = VM_ENTRY_LOAD_DEBUG_CONTROLS; opt = VM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_BNDCFGS; if ( adjust_vmx_controls ( min , opt , MSR_IA32_VMX_ENTRY_CTLS , & _vmentry_control ) < 0 )  static __init int adjust_vmx_controls(u32 ctl_min, u32 u32 msr, u32 *result) if ( ctl_min & ~ctl )  return - EIO ; return 0 ; return - EIO ; if ( ( vmx_msr_high & 0x1fff ) > PAGE_SIZE )  return - EIO ; if ( vmx_msr_high & ( 1u << 16 ) )  return - EIO ; if ( ( ( vmx_msr_high >> 18 ) & 15 ) != 6 )  return - EIO ; return 0 ; memcpy ( vmx_msr_bitmap_legacy_x2apic , vmx_msr_bitmap_legacy , PAGE_SIZE ); free_page ( ( unsigned long ) vmx_msr_bitmap_legacy_x2apic ); 